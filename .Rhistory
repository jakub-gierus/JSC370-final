rowSums(mat)
}
# Cumulative sum by row
fun2 <- function(mat) {
n <- nrow(mat)
k <- ncol(mat)
ans <- mat
for (i in 1:n) {
for (j in 2:k) {
ans[i,j] <- mat[i, j] + ans[i, j - 1]
}
}
ans
}
fun2alt <- function(mat) {
t(apply(mat, 1, cumsum))
}
# Use the data with this code
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)
# Test for the first
microbenchmark::microbenchmark(
fun1(dat),
fun1alt(dat), unit = "relative", check = "equivalent"
)
# Test for the second
microbenchmark::microbenchmark(
fun2(dat),
fun2alt(dat), unit = "relative", check = "equivalent"
)
library(parallel)
# Number of cores to use
no_cores <- detectCores() - 1
# Create a cluster of cores
cl <- makeCluster(no_cores)
# Set a common seed across all clusters for reproducibility
clusterSetRNGStream(cl, 1231)
# Parallel computation
system.time({
# Export sim_pi function to each cluster
clusterExport(cl, "sim_pi")
# Run sim_pi in parallel
ans <- parLapply(cl, 1:4000, sim_pi, n = 10000)
# Calculate the mean
print(mean(unlist(ans)))
# Stop the cluster
stopCluster(cl)
})
library(tidyverse)
library(modelr)
library(caret)
library(rpart)
library(randomForest)
library(gbm)
library(xgboost)
# Load the dataset
hitters <- read.csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/hitters/hitters.csv")
# Clean the data (remove NA values, etc.)
hitters <- na.omit(hitters)
set.seed(123) # For reproducibility
training_index <- createDataPartition(hitters$Salary, p = 0.7, list = FALSE)
training_set <- hitters[training_index, ]
testing_set <- hitters[-training_index, ]
categorical_vars <- sapply(training_set, is.character)
training_set[categorical_vars] <- lapply(training_set[categorical_vars], as.factor)
# Do the same for the testing set
testing_set[categorical_vars] <- lapply(testing_set[categorical_vars], as.factor)
# Fit regression tree
tree_model <- rpart(Salary ~ ., data = training_set, method = "anova")
# Prune the tree
printcp(tree_model) # Identify the optimal complexity parameter
pruned_tree <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"])
# Summarize the pruned tree
summary(pruned_tree)
set.seed(123)
bagging_model <- randomForest(Salary ~ ., data = training_set, importance = TRUE)
varImpPlot(bagging_model)
set.seed(123)
rf_model <- randomForest(Salary ~ ., data = training_set, importance = TRUE, ntree = 500)
varImpPlot(rf_model)
set.seed(123)
lambda_values <- seq(0.01, 0.1, by = 0.01)
mse_values <- sapply(lambda_values, function(lambda) {
model <- gbm(Salary ~ ., distribution = "gaussian", data = training_set, n.trees = 1000, interaction.depth = 1, shrinkage = lambda, verbose = FALSE)
mean((predict(model, training_set, n.trees = 1000) - training_set$Salary)^2)
})
# Plot of MSE vs. lambda
plot(lambda_values, mse_values, type = "b", xlab = "Lambda", ylab = "Training MSE")
# Variable importance for the model with the best lambda
best_lambda <- lambda_values[which.min(mse_values)]
best_boosting_model <- gbm(Salary ~ ., distribution = "gaussian", data = training_set, n.trees = 1000, interaction.depth = 1, shrinkage = best_lambda, verbose = FALSE)
summary(best_boosting_model)
set.seed(123)
params <- list(booster = "gbtree", objective = "reg:squarederror")
xg_train <- xgb.DMatrix(data = as.matrix(training_set[,-1]), label = training_set$Salary)
# Assuming 'training_set' and 'testing_set' are your data frames
# Convert training and testing sets to model matrices
training_matrix <- model.matrix(Salary ~ . - 1, data = training_set) # The '-1' removes the intercept
testing_matrix <- model.matrix(Salary ~ . - 1, data = testing_set)
# Ensure the target variable is numeric
training_labels <- as.numeric(training_set$Salary)
testing_labels <- as.numeric(testing_set$Salary)
# Create XGBoost DMatrices
xg_train <- xgb.DMatrix(data = training_matrix, label = training_labels)
xg_test <- xgb.DMatrix(data = testing_matrix, label = testing_labels)
# Now, you can proceed with your XGBoost training and prediction as before
# Example of setting up parameters and running a simple XGBoost model
params <- list(booster = "gbtree", objective = "reg:squarederror", eta = 0.1)
model <- xgb.train(params = params, data = xg_train, nrounds = 1000)
# Predictions
preds <- predict(model, xg_test)
mse <- mean((preds - testing_labels)^2)
print(mse)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
print(mse_bagging)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
print(mse_bagging)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
print(mse_rf)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
print(mse_bagging)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
print(mse_rf)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
print(mse_boost)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
print(mse_bagging)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
print(mse_rf)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
print(mse_boost)
predictions_xgb <- predict(model, xg_test)
# Calculate MSE
mse_xgb <- mean((predictions_xgb - testing_labels)^2)
print(mse_xgb)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
print(mse_bagging)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
print(mse_rf)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
print(mse_boost)
predictions_xgb <- predict(model, xg_test)
# Calculate MSE
mse_xgb <- mean((predictions_xgb - testing_labels)^2)
print("XGBoost: ", mse_xgb)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
print(mse_bagging)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
predictions_xgb <- predict(model, xg_test)
# Calculate MSE
mse_xgb <- mean((predictions_xgb - testing_labels)^2)
print(paste("Regression Tree MSE:", mse_tree))
# Print MSE for Bagging
print(paste("Bagging MSE:", mse_bagging))
# Print MSE for Random Forest
print(paste("Random Forest MSE:", mse_rf))
# Print MSE for Boosting
print(paste("Boosting MSE:", mse_boost))
# Print MSE for XGBoost
print(paste("XGBoost MSE:", mse_xgb))
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
predictions_xgb <- predict(model, xg_test)
# Calculate MSE
mse_xgb <- mean((predictions_xgb - testing_labels)^2)
print(paste("Regression Tree MSE:", mse_tree))
# Print MSE for Bagging
print(paste("Bagging MSE:", mse_bagging))
# Print MSE for Random Forest
print(paste("Random Forest MSE:", mse_rf))
# Print MSE for Boosting
print(paste("Boosting MSE:", mse_boost))
# Print MSE for XGBoost
print(paste("XGBoost MSE:", mse_xgb))
importance_bagging <- importance(bagging_model)
# Ordering the importance
ordered_importance_bagging <- sort(importance_bagging, decreasing = TRUE)
print(ordered_importance_bagging)
importance_bagging <- importance(bagging_model)
normalized_importance_bagging <- importance_bagging / sum(importance_bagging)
# Random Forest - Variable Importance
importance_rf <- importance(rf_model)
normalized_importance_rf <- importance_rf[, "MeanDecreaseGini"] / sum(importance_rf[, "MeanDecreaseGini"])
importance_bagging <- importance(bagging_model)
normalized_importance_bagging <- importance_bagging / sum(importance_bagging)
# Random Forest - Variable Importance
importance_rf <- importance(rf_model)
normalized_importance_rf <- importance(rf_model)[, 1] / sum(importance(rf_model)[, 1])
# Boosting - Variable Importance
importance_boosting <- summary(best_boosting_model, n.trees = 1000)
normalized_importance_boosting <- importance_boosting / sum(importance_boosting)
importance_bagging <- importance(bagging_model)
normalized_importance_bagging <- importance_bagging / sum(importance_bagging)
# Random Forest - Variable Importance
importance_rf <- importance(rf_model)
normalized_importance_rf <- importance(rf_model)[, 1] / sum(importance(rf_model)[, 1])
if (exists("best_boosting_model")) {
importance_boosting <- summary(best_boosting_model, n.trees = 1000)$rel.inf
normalized_importance_boosting <- importance_boosting / sum(importance_boosting)
} else {
normalized_importance_boosting <- rep(NA, length(var_names)) # Placeholder if model does not exist
}
# For XGBoost - Adjusting for variable names and ensuring compatibility
if (exists("model")) {
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
normalized_importance_xgb <- setNames(importance_xgb$Gain / sum(importance_xgb$Gain), importance_xgb$Feature)
} else {
normalized_importance_xgb <- rep(NA, length(var_names)) # Placeholder if model does not exist
}
# Combine into a single data frame for comparison
var_names <- rownames(importance_rf)
importance_df <- data.frame(
Variable = var_names,
Bagging = normalized_importance_bagging[var_names],
RandomForest = normalized_importance_rf[var_names],
Boosting = normalized_importance_boosting[names(normalized_importance_boosting) %in% var_names],
XGBoost = normalized_importance_xgb[match(var_names, importance_xgb$Feature)]
)
importance_bagging <- importance(bagging_model)
normalized_importance_bagging <- importance_bagging / sum(importance_bagging)
# Random Forest - Variable Importance
importance_rf <- importance(rf_model)
normalized_importance_rf <- importance(rf_model)[, 1] / sum(importance(rf_model)[, 1])
if (exists("best_boosting_model")) {
importance_boosting <- summary(best_boosting_model, n.trees = 1000)$rel.inf
normalized_importance_boosting <- importance_boosting / sum(importance_boosting)
} else {
normalized_importance_boosting <- rep(NA, length(var_names)) # Placeholder if model does not exist
}
# For XGBoost - Adjusting for variable names and ensuring compatibility
if (exists("model")) {
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
normalized_importance_xgb <- setNames(importance_xgb$Gain / sum(importance_xgb$Gain), importance_xgb$Feature)
} else {
normalized_importance_xgb <- rep(NA, length(var_names)) # Placeholder if model does not exist
}
# Combine into a single data frame for comparison
importance_df <- data.frame(
Variable = rownames(importance(rf_model)),
Bagging = normalized_importance_bagging[rownames(importance(rf_model))],
RandomForest = normalized_importance_rf,
Boosting = normalized_importance_boosting[names(normalized_importance_boosting)],
XGBoost = normalized_importance_xgb[names(normalized_importance_xgb)]
)
library(xgboost) # Ensure the xgboost package is loaded for xgb.importance
# Extract variable importance
importance_bagging <- importance(bagging_model)
importance_rf <- importance(rf_model)
importance_boosting <- summary(best_boosting_model, n.trees = 1000, plot = FALSE)
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
# Convert to data frames for cohesive presentation
df_importance_bagging <- data.frame(Feature = rownames(importance_bagging), Importance = importance_bagging[, "MeanDecreaseGini"], Model = "Bagging")
library(xgboost) # Ensure the xgboost package is loaded for xgb.importance
# Extract variable importance
importance_bagging <- importance(bagging_model)
importance_rf <- importance(rf_model)
importance_boosting <- summary(best_boosting_model, n.trees = 1000, plot = FALSE)
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
# Convert to data frames for cohesive presentation
importance_bagging
df_importance_bagging <- data.frame(Feature = rownames(importance_bagging), Importance = importance_bagging[, "MeanDecreaseGini"], Model = "Bagging")
library(xgboost) # Ensure the xgboost package is loaded for xgb.importance
# Assuming you've already trained your models: bagging_model, rf_model, best_boosting_model, and model (XGBoost)
# Extract variable importance
importance_bagging <- importance(bagging_model, type = 1)  # %IncMSE
importance_rf <- importance(rf_model, type = 1)            # %IncMSE
importance_boosting <- summary(best_boosting_model, n.trees = 1000, plot = FALSE)
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
# Convert to data frames for cohesive presentation
df_importance_bagging <- data.frame(Feature = rownames(importance_bagging), Importance = importance_bagging[, "%IncMSE"], Model = "Bagging")
df_importance_rf <- data.frame(Feature = rownames(importance_rf), Importance = importance_rf[, "%IncMSE"], Model = "Random Forest")
df_importance_boosting <- data.frame(Feature = names(importance_boosting), Importance = importance_boosting, Model = "Boosting")
library(xgboost) # Make sure xgboost is loaded
# Correct extraction of variable importance for the boosting model
importance_boosting <- summary(best_boosting_model, n.trees = 1000, plot = FALSE)
# Ensure this is a data frame with the correct columns. If summary() doesn't work as expected, adjust accordingly.
# Assuming importance_boosting is now a data frame with the correct structure:
df_importance_boosting <- data.frame(Feature = row.names(importance_boosting),
Importance = importance_boosting$rel.inf,
Model = "Boosting")
# If the above assumption about the structure of importance_boosting is incorrect, adjust to match its actual structure.
# Proceed with creating data frames for bagging, RF, and XGBoost as before
df_importance_bagging <- data.frame(Feature = rownames(importance_bagging), Importance = importance_bagging[, "%IncMSE"], Model = "Bagging")
df_importance_rf <- data.frame(Feature = rownames(importance_rf), Importance = importance_rf[, "%IncMSE"], Model = "Random Forest")
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
df_importance_xgb <- data.frame(Feature = importance_xgb$Feature, Importance = importance_xgb$Gain, Model = "XGBoost")
# Combine into a single data frame
combined_importance <- rbind(df_importance_bagging, df_importance_rf, df_importance_boosting, df_importance_xgb)
# Normalize the importance to make them comparable
combined_importance$NormalizedImportance <- with(combined_importance, Importance / max(Importance))
# Print or plot the results
print(combined_importance)
# Total row sums
fun1 <- function(mat) {
n <- nrow(mat)
ans <- double(n)
for (i in 1:n) {
ans[i] <- sum(mat[i, ])
}
ans
}
fun1alt <- function(mat) {
rowSums(mat)
}
# Cumulative sum by row
fun2 <- function(mat) {
n <- nrow(mat)
k <- ncol(mat)
ans <- mat
for (i in 1:n) {
for (j in 2:k) {
ans[i,j] <- mat[i, j] + ans[i, j - 1]
}
}
ans
}
fun2alt <- function(mat) {
t(apply(mat, 1, cumsum))
}
# Use the data with this code
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)
# Test for the first
microbenchmark::microbenchmark(
fun1(dat),
fun1alt(dat), unit = "relative", check = "equivalent"
)
# Test for the second
microbenchmark::microbenchmark(
fun2(dat),
fun2alt(dat), unit = "relative", check = "equivalent"
)
library(parallel)
# Number of cores to use
no_cores <- detectCores() - 1
# Create a cluster of cores
cl <- makeCluster(no_cores)
# Set a common seed across all clusters for reproducibility
clusterSetRNGStream(cl, 1231)
# Parallel computation
system.time({
# Export sim_pi function to each cluster
clusterExport(cl, "sim_pi")
# Run sim_pi in parallel
ans <- parLapply(cl, 1:4000, sim_pi, n = 10000)
# Calculate the mean
print(mean(unlist(ans)))
# Stop the cluster
stopCluster(cl)
})
library(tidyverse)
library(modelr)
library(caret)
library(rpart)
library(randomForest)
library(gbm)
library(xgboost)
# Load the dataset
hitters <- read.csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/hitters/hitters.csv")
# Clean the data (remove NA values, etc.)
hitters <- na.omit(hitters)
set.seed(123) # For reproducibility
training_index <- createDataPartition(hitters$Salary, p = 0.7, list = FALSE)
training_set <- hitters[training_index, ]
testing_set <- hitters[-training_index, ]
categorical_vars <- sapply(training_set, is.character)
training_set[categorical_vars] <- lapply(training_set[categorical_vars], as.factor)
# Do the same for the testing set
testing_set[categorical_vars] <- lapply(testing_set[categorical_vars], as.factor)
# Fit regression tree
tree_model <- rpart(Salary ~ ., data = training_set, method = "anova")
# Prune the tree
printcp(tree_model) # Identify the optimal complexity parameter
pruned_tree <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"])
# Summarize the pruned tree
summary(pruned_tree)
set.seed(123)
bagging_model <- randomForest(Salary ~ ., data = training_set, importance = TRUE)
varImpPlot(bagging_model)
set.seed(123)
rf_model <- randomForest(Salary ~ ., data = training_set, importance = TRUE, ntree = 500)
varImpPlot(rf_model)
set.seed(123)
lambda_values <- seq(0.01, 0.1, by = 0.01)
mse_values <- sapply(lambda_values, function(lambda) {
model <- gbm(Salary ~ ., distribution = "gaussian", data = training_set, n.trees = 1000, interaction.depth = 1, shrinkage = lambda, verbose = FALSE)
mean((predict(model, training_set, n.trees = 1000) - training_set$Salary)^2)
})
# Plot of MSE vs. lambda
plot(lambda_values, mse_values, type = "b", xlab = "Lambda", ylab = "Training MSE")
# Variable importance for the model with the best lambda
best_lambda <- lambda_values[which.min(mse_values)]
best_boosting_model <- gbm(Salary ~ ., distribution = "gaussian", data = training_set, n.trees = 1000, interaction.depth = 1, shrinkage = best_lambda, verbose = FALSE)
summary(best_boosting_model)
# Assuming 'training_set' and 'testing_set' are your data frames
# Convert training and testing sets to model matrices
training_matrix <- model.matrix(Salary ~ . - 1, data = training_set) # The '-1' removes the intercept
testing_matrix <- model.matrix(Salary ~ . - 1, data = testing_set)
# Ensure the target variable is numeric
training_labels <- as.numeric(training_set$Salary)
testing_labels <- as.numeric(testing_set$Salary)
# Create XGBoost DMatrices
xg_train <- xgb.DMatrix(data = training_matrix, label = training_labels)
xg_test <- xgb.DMatrix(data = testing_matrix, label = testing_labels)
# Now, you can proceed with your XGBoost training and prediction as before
# Example of setting up parameters and running a simple XGBoost model
params <- list(booster = "gbtree", objective = "reg:squarederror", eta = 0.1)
model <- xgb.train(params = params, data = xg_train, nrounds = 1000)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
predictions_xgb <- predict(model, xg_test)
# Calculate MSE
mse_xgb <- mean((predictions_xgb - testing_labels)^2)
print(paste("Regression Tree MSE:", mse_tree))
# Print MSE for Bagging
print(paste("Bagging MSE:", mse_bagging))
# Print MSE for Random Forest
print(paste("Random Forest MSE:", mse_rf))
# Print MSE for Boosting
print(paste("Boosting MSE:", mse_boost))
# Print MSE for XGBoost
print(paste("XGBoost MSE:", mse_xgb))
library(xgboost)
importance_bagging <- importance(bagging_model, type = 1)  # %IncMSE
importance_rf <- importance(rf_model, type = 1)            # %IncMSE
df_importance_bagging <- data.frame(Feature = rownames(importance_bagging), Importance = importance_bagging[, "%IncMSE"], Model = "Bagging")
df_importance_rf <- data.frame(Feature = rownames(importance_rf), Importance = importance_rf[, "%IncMSE"], Model = "Random Forest")
df_importance_xgb <- data.frame(Feature = importance_xgb$Feature, Importance = importance_xgb$Gain, Model = "XGBoost")
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
importance_boosting <- summary(best_boosting_model, n.trees = 1000, plot = FALSE)
df_importance_boosting <- data.frame(Feature = row.names(importance_boosting),
Importance = importance_boosting$rel.inf,
Model = "Boosting")
df_importance_bagging <- data.frame(Feature = rownames(importance_bagging), Importance = importance_bagging[, "%IncMSE"], Model = "Bagging")
df_importance_rf <- data.frame(Feature = rownames(importance_rf), Importance = importance_rf[, "%IncMSE"], Model = "Random Forest")
importance_xgb <- xgb.importance(feature_names = colnames(training_matrix), model = model)
df_importance_xgb <- data.frame(Feature = importance_xgb$Feature, Importance = importance_xgb$Gain, Model = "XGBoost")
# Combine into a single data frame
combined_importance <- rbind(df_importance_bagging, df_importance_rf, df_importance_boosting, df_importance_xgb)
# Normalize the importance to make them comparable
combined_importance$NormalizedImportance <- with(combined_importance, Importance / max(Importance))
# Print or plot the results
print(combined_importance)
predictions_bagging <- predict(bagging_model, testing_set)
# Calculate MSE
mse_bagging <- mean((predictions_bagging - testing_set$Salary)^2)
predictions_rf <- predict(rf_model, testing_set)
# Calculate MSE
mse_rf <- mean((predictions_rf - testing_set$Salary)^2)
predictions_boost <- predict(best_boosting_model, testing_set, n.trees = 1000)
# Calculate MSE
mse_boost <- mean((predictions_boost - testing_set$Salary)^2)
predictions_xgb <- predict(model, xg_test)
predictions_tree <- predict(pruned_tree, testing_set)
# Calculate MSE
mse_tree <- mean((predictions_tree - testing_set$Salary)^2)
# Calculate MSE
mse_xgb <- mean((predictions_xgb - testing_labels)^2)
print(paste("Regression Tree MSE:", mse_tree))
# Print MSE for Bagging
print(paste("Bagging MSE:", mse_bagging))
# Print MSE for Random Forest
print(paste("Random Forest MSE:", mse_rf))
# Print MSE for Boosting
print(paste("Boosting MSE:", mse_boost))
# Print MSE for XGBoost
print(paste("XGBoost MSE:", mse_xgb))
