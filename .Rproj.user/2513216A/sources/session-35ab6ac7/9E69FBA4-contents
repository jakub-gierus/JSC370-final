---
title: "Lab 08 - Text Mining/NLP"
output: html_document
author: "Jakub Gierus"
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://github.com/JSC370/JSC370-2024/tree/main/data/medical_transcriptions.

This markdown document should be rendered using `github_document` document.



### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`. Install `wordcloud`, `tm`, and `topicmodels` if you don't alreadyh have them.

```{r}
```


### Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r eval=TRUE}
library(tidytext)
library(tidyverse)
library(wordcloud)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?


```{r}
names(mt_samples)
```

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r eval=TRUE}
mt_samples_counted <- mt_samples %>%
  count(medical_specialty, sort = TRUE)

ggplot(mt_samples_counted, aes(x = reorder(medical_specialty, n), y = n)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # Rotate x labels for better readability
  labs(x = "Medical Specialty", y = "Count", title = "Distribution of Medical Specialties")
```
The medical specialties are not uniformly distributed, with surgery by far being the most common specialty. There does seem to be some overlap, but no two specialties are the same.
---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r eval=TRUE}
tokens <- mt_samples %>%
  unnest_tokens(word, transcription)

word_counts <- tokens %>%
  count(word, sort = TRUE)

word_counts %>%
  top_n(20, wt = n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() + 
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Frequent Words in Transcriptions")

wordcloud(words = word_counts$word, freq = word_counts$n, min.freq = 1,
          max.words = 100, random.order = FALSE, rot.per = 0.35, 
          colors = brewer.pal(8, "Dark2"))
```
We see just the very common words in any English text, such as "the", "and", "was". There are little insights to be gained, as I would assume that any English text would yield very similar results.
---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords(use stopwords package)
- Bonus points if you remove numbers as well (use regex)

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

```{r eval=TRUE}
head(stopwords("english"))
length(stopwords("english"))


tokens <- mt_samples %>%
  unnest_tokens(word, transcription) %>%
  filter(!word %in% stopwords('english')) %>% 
  filter(!str_detect(word, "^[0-9]+$")) 

word_counts <- tokens %>%
  count(word, sort = TRUE)

word_counts %>%
  top_n(20, wt = n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Frequent Words in Transcriptions (without Stopwords and Numbers)")

wordcloud(words = word_counts$word, freq = word_counts$n, min.freq = 1,
          max.words = 100, random.order = FALSE, rot.per = 0.35, 
          colors = brewer.pal(8, "Dark2"))


```
Getting rid of the stopwords tells us that this is part of medical text, as the most frequent words are "patient", "procedure", "history" etc.

---



# Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r eval=TRUE}
custom_stopwords <- c(stopwords("en"))

tokens_bigram <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(bigram, transcription, token = "ngrams", n = 2) %>%
  filter(!str_detect(bigram, paste0("^(", paste(custom_stopwords, collapse="|"), ")\\s")) & 
           !str_detect(bigram, paste0("\\s(", paste(custom_stopwords, collapse="|"), ")$"))) 

bigram_counts <- tokens_bigram %>%
  count(bigram, sort = TRUE)

ggplot(bigram_counts[1:20,], aes(x = reorder(bigram, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Bi-gram", y = "Frequency", title = "Top 20 Most Frequent Bi-grams in Transcriptions")
```

```{r}
tokens_trigram <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(trigram, transcription, token = "ngrams", n = 3) %>%
  filter(!str_detect(trigram, paste0("^(", paste(custom_stopwords, collapse="|"), ")\\s")) &
           !str_detect(trigram, paste0("\\s(", paste(custom_stopwords, collapse="|"), ")$")))
trigram_counts <- tokens_trigram %>%
  count(trigram, sort = TRUE)

ggplot(trigram_counts[1:20,], aes(x = reorder(trigram, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Tri-gram", y = "Frequency", title = "Top 20 Most Frequent Tri-grams in Transcriptions")
```
The trigram has very different words than the bigram frequency plot. One thing to notice is that the middle word in the trigram frequency plot is usually a stopword.

---



# Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r eval=TRUE}
library(stringr)
diagnosis_bigrams <- tokens_bigram %>%
  filter(str_detect(bigram, "diagnosis"))

diagnosis_words <- diagnosis_bigrams %>%
  mutate(before_diagnosis = if_else(str_detect(bigram, "\\b\\w+ diagnosis"), str_extract(bigram, "\\b\\w+(?= diagnosis)"), NA_character_),
         after_diagnosis = if_else(str_detect(bigram, "diagnosis \\w+\\b"), str_extract(bigram, "(?<=diagnosis )\\w+\\b"), NA_character_)) %>%
  select(before_diagnosis, after_diagnosis) %>%
  pivot_longer(cols = c(before_diagnosis, after_diagnosis), values_to = "word") %>%
  filter(!is.na(word))


word_counts <- diagnosis_words %>%
  count(word, sort = TRUE)

top_20_words <- word_counts %>%
  top_n(20, wt = n)

ggplot(top_20_words, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() + 
  labs(x = "Word", y = "Frequency", title = "Top 20 Words Before and After 'Diagnosis'")
```

---


# Question 6: Words by Specialties

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?


```{r eval=TRUE}
cleaned_tokens <- mt_samples %>%
  unnest_tokens(word, transcription) %>%
  anti_join(get_stopwords(), by = "word")


top_words_by_specialty <- cleaned_tokens %>%
  group_by(medical_specialty) %>%
  count(word, sort = TRUE) %>%
  top_n(5, n) %>%
  ungroup() %>%
  arrange(medical_specialty, desc(n))

print(top_words_by_specialty)
```


# Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r eval=TRUE}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(tm) 

data("stop_words")

transcripts_dtm <- mt_samples %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by = "word") %>%
  count(document = row_number(), word) %>%
  cast_dtm(document, word, n)

transcripts_lda <- LDA(transcripts_dtm, k = 5, control = list(seed = 1234))

topics <- tidy(transcripts_lda, matrix = "beta")

top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(term = reorder_within(term, beta, topic))

ggplot(top_terms, aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_x_reordered() +
  coord_flip() +
  labs(y = "Beta", x = "Term", title = "Top Terms in Each Topic from LDA Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```



# Deliverables

1. Questions 1-7 answered, raw .Rmd file and pdf or html output uploaded to Quercus
